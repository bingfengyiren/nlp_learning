## Attention Is All You Need

### 摘要
优秀的序列翻译模型都是基于复杂循环或者卷积神经网络，然后加上编码器与解码器实现的。表现最好的模型还通过注意力机制来连接编码与解码器。我们摒弃了传统的循环、卷积网络，提出了一种只有注意力机制的全新网络结构：transformer。我们再两个机器翻译任务上进行了实验，实验结果显示在算法效果是有优势的，除此之前它还有更高的并行化，需要的训练时间也更少。实验表明在英语翻译

### 1. 引言

### 2. 背景

### 3. 模型结构

##### 3.1 多层编码与解码器

##### 3.2 注意力机制

##### 3.2.1 Scaled Dot-Product Attention

##### 3.2.2 多头注意力机制

##### 3.2.3 注意力机制在我们模型中的应用

##### 3.3 Position-wise Feed-Forward Networks

##### 3.4 Embeddings and Softmax

##### 3.5 位置编码

### 4. 为什么需要自注意力机制

### 5. 训练

##### 5.1 训练数据与分块

##### 5.2 硬件与训练时间

##### 5.3 优化

##### 5.4 正则化

### 6. 结果
##### 6.1 机器翻译

##### 6.2 模型变量

##### 6.3 英语句法分析

### 7. 总结
本文我们介绍了transformer算法，这是第一个完全基于注意力机制的序列转换模型，用多头自注意力机制代替编解码器中最常用的循环层。
